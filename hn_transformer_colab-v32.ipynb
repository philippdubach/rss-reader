{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HN Success Predictor V3.2 - Clean Architecture\n",
    "\n",
    "**Architecture:** RoBERTa + Label-Smoothed CrossEntropy ‚Üí Isotonic Calibration\n",
    "\n",
    "**Key Changes from V3.1 (based on statistical analysis):**\n",
    "\n",
    "### Problem Diagnosis\n",
    "1. **Data Contamination**: V3.1 mislabeled medium posts (30-50 pts ‚Üí label=1), corrupting the decision boundary\n",
    "2. **Focal Loss Overcorrection**: Œ±=0.25 caused 3.5x more FN than FP (760 vs 219) - model too conservative\n",
    "3. **Stacking Complexity**: LightGBM added noise; RoBERTa dominated at 84.4% importance anyway\n",
    "4. **V1 Outperformed V3.1**: Simpler DistilBERT (ROC 0.77) beat complex stacking (ROC 0.72)\n",
    "\n",
    "### V3.2 Design Principles\n",
    "- **Clean Labels**: Strict binary labeling (‚â•100 pts = 1, else = 0) - no medium-post contamination\n",
    "- **V1-style Data Distribution**: High (100+), Medium (20-99‚Üí0), Low (1-19‚Üí0) for proper boundary learning\n",
    "- **Standard Loss**: Label-smoothed CrossEntropy with inverse-frequency class weights (statistically principled)\n",
    "- **No Stacking**: Pure RoBERTa end-to-end training (Occam's razor)\n",
    "- **Keep Isotonic Calibration**: V3.1's ECE=0.01 was genuinely excellent\n",
    "\n",
    "### Expected Improvements\n",
    "| Metric | V3.1 | V3.2 Target |\n",
    "|--------|------|-------------|\n",
    "| ROC AUC | 0.715 | **‚â•0.77** (match V1) |\n",
    "| Precision | 0.589 | **‚â•0.62** |\n",
    "| ECE | 0.013 | **‚â§0.05** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate scikit-learn pandas tqdm lightgbm seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, precision_recall_curve, roc_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from datasets import Dataset\n",
    "import lightgbm as lgb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# V3.2 CONFIGURATION - Statistically Principled Settings\n",
    "# =============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"roberta-base\"  # Keep RoBERTa for fair comparison; can try distilbert-base-uncased\n",
    "MAX_LENGTH = 128  # Sufficient for HN titles (avg ~50 chars)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5  # Standard for transformer fine-tuning\n",
    "WEIGHT_DECAY = 0.01  # L2 regularization\n",
    "EPOCHS = 4  # Increased from 3; early stopping will prevent overfitting\n",
    "WARMUP_RATIO = 0.1  # Linear warmup for first 10% of training\n",
    "SEED = 42\n",
    "\n",
    "# Label smoothing (reduces overconfidence, improves calibration)\n",
    "# Interpretation: \"I'm 90% sure of the label, 10% uncertain\"\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# Data settings - V1-style distribution for proper boundary learning\n",
    "POSTS_PER_CATEGORY = 25000  # Balanced across categories\n",
    "HIT_THRESHOLD = 100  # Clear, unambiguous threshold\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V3.2 Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Loss: CrossEntropy + Label Smoothing ({LABEL_SMOOTHING})\")\n",
    "print(f\"  Epochs: {EPOCHS} (with early stopping)\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Hit Threshold: ‚â•{HIT_THRESHOLD} points\")\n",
    "print(f\"  Posts per category: {POSTS_PER_CATEGORY:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_hn_posts(min_points, max_points, target_count, months_back=24):\n",
    "    \"\"\"Fetch HN posts from Algolia API with time-based pagination.\"\"\"\n",
    "    base_url = \"https://hn.algolia.com/api/v1/search_by_date\"\n",
    "    all_posts = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    if max_points:\n",
    "        points_filter = f\"points>={min_points},points<={max_points}\"\n",
    "        desc = f\"{min_points}-{max_points} pts\"\n",
    "    else:\n",
    "        points_filter = f\"points>={min_points}\"\n",
    "        desc = f\"{min_points}+ pts\"\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    pbar = tqdm(range(months_back), desc=f\"Fetching {desc}\")\n",
    "\n",
    "    for month_offset in pbar:\n",
    "        month_end = end_date - timedelta(days=30 * month_offset)\n",
    "        month_start = month_end - timedelta(days=30)\n",
    "        \n",
    "        # Paginate within each month (up to 10 pages of 1000)\n",
    "        for page in range(10):\n",
    "            if len(all_posts) >= target_count:\n",
    "                break\n",
    "                \n",
    "            params = {\n",
    "                \"tags\": \"story\",\n",
    "                \"numericFilters\": f\"{points_filter},created_at_i>={int(month_start.timestamp())},created_at_i<={int(month_end.timestamp())}\",\n",
    "                \"hitsPerPage\": 1000,\n",
    "                \"page\": page,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(base_url, params=params, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                data = resp.json()\n",
    "                hits = data.get(\"hits\", [])\n",
    "                \n",
    "                if not hits:\n",
    "                    break\n",
    "\n",
    "                for hit in hits:\n",
    "                    post_id = hit.get(\"objectID\")\n",
    "                    if post_id and post_id not in seen_ids:\n",
    "                        seen_ids.add(post_id)\n",
    "                        url = hit.get(\"url\", \"\")\n",
    "                        domain = \"\"\n",
    "                        if url:\n",
    "                            try:\n",
    "                                domain = urlparse(url).netloc.replace(\"www.\", \"\")\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        all_posts.append({\n",
    "                            \"title\": hit.get(\"title\", \"\"),\n",
    "                            \"url\": url,\n",
    "                            \"domain\": domain,\n",
    "                            \"score\": hit.get(\"points\", 0),\n",
    "                            \"submitter\": hit.get(\"author\", \"\"),\n",
    "                            \"timestamp\": hit.get(\"created_at\", \"\"),\n",
    "                            \"num_comments\": hit.get(\"num_comments\", 0),\n",
    "                        })\n",
    "\n",
    "                pbar.set_postfix({\"total\": len(all_posts)})\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "\n",
    "        if len(all_posts) >= target_count:\n",
    "            break\n",
    "\n",
    "    return all_posts[:target_count]\n",
    "\n",
    "print(\"‚úì Data fetching function defined (time-based pagination)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA FETCHING - V1-style distribution (Critical Fix)\n",
    "# =============================================================================\n",
    "# \n",
    "# V3.1 BUG: Medium posts (30-50) were labeled as hits, contaminating the \n",
    "# positive class with posts that objectively did NOT reach 100 points.\n",
    "#\n",
    "# V3.2 FIX: Three clear categories, all labeled by strict threshold:\n",
    "#   - HIGH:   100+ points  ‚Üí label = 1 (true hits)\n",
    "#   - MEDIUM: 20-99 points ‚Üí label = 0 (near-misses, critical for boundary)\n",
    "#   - LOW:    1-19 points  ‚Üí label = 0 (clear misses)\n",
    "#\n",
    "# This ensures the model learns the TRUE decision boundary at 100 points.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Fetching HN posts (V1-style distribution)...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Category 1: HITS (score >= 100) - TRUE POSITIVES\n",
    "print(f\"\\nüìà Fetching {POSTS_PER_CATEGORY:,} HIT posts (score ‚â• {HIT_THRESHOLD})...\")\n",
    "hits = fetch_hn_posts(\n",
    "    min_points=HIT_THRESHOLD, \n",
    "    max_points=None, \n",
    "    target_count=POSTS_PER_CATEGORY, \n",
    "    months_back=24\n",
    ")\n",
    "print(f\"   ‚Üí Got {len(hits):,} posts (mean score: {np.mean([p['score'] for p in hits]):.0f})\")\n",
    "\n",
    "# Category 2: MEDIUM (score 20-99) - NEAR-MISS NEGATIVES (critical for boundary learning)\n",
    "print(f\"\\nüìä Fetching {POSTS_PER_CATEGORY:,} MEDIUM posts (score 20-99)...\")\n",
    "medium = fetch_hn_posts(\n",
    "    min_points=20, \n",
    "    max_points=99, \n",
    "    target_count=POSTS_PER_CATEGORY, \n",
    "    months_back=24\n",
    ")\n",
    "print(f\"   ‚Üí Got {len(medium):,} posts (mean score: {np.mean([p['score'] for p in medium]):.0f})\")\n",
    "\n",
    "# Category 3: LOW (score 1-19) - CLEAR NEGATIVES\n",
    "print(f\"\\nüìâ Fetching {POSTS_PER_CATEGORY:,} LOW posts (score 1-19)...\")\n",
    "low = fetch_hn_posts(\n",
    "    min_points=1, \n",
    "    max_points=19, \n",
    "    target_count=POSTS_PER_CATEGORY, \n",
    "    months_back=24\n",
    ")\n",
    "print(f\"   ‚Üí Got {len(low):,} posts (mean score: {np.mean([p['score'] for p in low]):.0f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# STRICT LABELING - No contamination\n",
    "# =============================================================================\n",
    "for p in hits:\n",
    "    p[\"label\"] = 1  # Only true hits get label=1\n",
    "\n",
    "for p in medium:\n",
    "    p[\"label\"] = 0  # ALL medium posts are label=0 (not hits!)\n",
    "\n",
    "for p in low:\n",
    "    p[\"label\"] = 0  # Clear misses\n",
    "\n",
    "# Combine datasets\n",
    "all_posts = hits + medium + low\n",
    "df = pd.DataFrame(all_posts)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Data quality checks\n",
    "n_hits = (df[\"label\"] == 1).sum()\n",
    "n_misses = (df[\"label\"] == 0).sum()\n",
    "hit_rate = n_hits / len(df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total posts:     {len(df):,}\")\n",
    "print(f\"  Hits (label=1):  {n_hits:,} ({hit_rate:.1%})\")\n",
    "print(f\"  Misses (label=0): {n_misses:,} ({1-hit_rate:.1%})\")\n",
    "print(f\"  Class ratio:     1:{n_misses/n_hits:.1f} (neg:pos)\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  Score distribution by label:\")\n",
    "print(f\"    Hits:   min={df[df['label']==1]['score'].min()}, max={df[df['label']==1]['score'].max()}, mean={df[df['label']==1]['score'].mean():.0f}\")\n",
    "print(f\"    Misses: min={df[df['label']==0]['score'].min()}, max={df[df['label']==0]['score'].max()}, mean={df[df['label']==0]['score'].mean():.0f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify no label contamination\n",
    "contaminated = df[(df[\"label\"] == 1) & (df[\"score\"] < HIT_THRESHOLD)]\n",
    "if len(contaminated) > 0:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {len(contaminated)} posts labeled as hits but score < {HIT_THRESHOLD}\")\n",
    "else:\n",
    "    print(\"‚úì Label integrity verified: No contamination detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MINIMAL FEATURE ENGINEERING - Only Genuinely Useful Features\n",
    "# =============================================================================\n",
    "#\n",
    "# V3.1 had 25 features but RoBERTa dominated at 84.4%. Most features added noise.\n",
    "# V3.2: We keep only features that are:\n",
    "#   1. Content-based (not identity-based like author/domain history)\n",
    "#   2. Proven useful in V3.1's feature importance analysis\n",
    "#   3. Not learnable from the title text alone (complementary to transformer)\n",
    "#\n",
    "# These features will be used for a simple calibration adjustment, not stacking.\n",
    "# =============================================================================\n",
    "\n",
    "class MinimalFeatureEngineer:\n",
    "    \"\"\"Extract only features that complement the transformer's semantic understanding.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"No fitting needed - all features are rule-based.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Extract minimal, interpretable features.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # ----- Post Type Indicators (high signal, not in title semantics) -----\n",
    "        features[\"is_show_hn\"] = df[\"title\"].str.contains(r\"^Show HN:\", case=False, regex=True).fillna(False).astype(int)\n",
    "        features[\"is_ask_hn\"] = df[\"title\"].str.contains(r\"^Ask HN:\", case=False, regex=True).fillna(False).astype(int)\n",
    "        \n",
    "        # ----- URL Type (categorical, complements title) -----\n",
    "        features[\"has_url\"] = (df[\"url\"].str.len() > 0).fillna(False).astype(int)\n",
    "        features[\"is_github\"] = df[\"url\"].str.contains(\"github.com\", case=False).fillna(False).astype(int)\n",
    "        features[\"is_pdf\"] = df[\"url\"].str.contains(r\"\\.pdf($|\\?)\", case=False, regex=True).fillna(False).astype(int)\n",
    "        \n",
    "        # ----- Title Structure (not semantics) -----\n",
    "        features[\"title_length\"] = df[\"title\"].str.len().fillna(0)\n",
    "        features[\"title_words\"] = df[\"title\"].str.split().str.len().fillna(0)\n",
    "        features[\"has_question\"] = df[\"title\"].str.contains(r\"\\?\", regex=True).fillna(False).astype(int)\n",
    "        features[\"has_number\"] = df[\"title\"].str.contains(r\"\\d\", regex=True).fillna(False).astype(int)\n",
    "        \n",
    "        # ----- Time Features (cyclical encoding for posting time effects) -----\n",
    "        if \"timestamp\" in df.columns:\n",
    "            ts = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "            hour = ts.dt.hour.fillna(12)  # Default to noon if missing\n",
    "            dow = ts.dt.dayofweek.fillna(2)  # Default to Wednesday\n",
    "            \n",
    "            # Cyclical encoding preserves continuity (23:00 is close to 00:00)\n",
    "            features[\"hour_sin\"] = np.sin(2 * np.pi * hour / 24)\n",
    "            features[\"hour_cos\"] = np.cos(2 * np.pi * hour / 24)\n",
    "            features[\"dow_sin\"] = np.sin(2 * np.pi * dow / 7)\n",
    "            features[\"dow_cos\"] = np.cos(2 * np.pi * dow / 7)\n",
    "        \n",
    "        return features.fillna(0)\n",
    "\n",
    "# Initialize and fit (no-op but maintains interface)\n",
    "feature_engineer = MinimalFeatureEngineer()\n",
    "feature_engineer.fit(train_df)\n",
    "\n",
    "# Transform\n",
    "train_features = feature_engineer.transform(train_df)\n",
    "val_features = feature_engineer.transform(val_df)\n",
    "test_features = feature_engineer.transform(test_df)\n",
    "\n",
    "print(f\"‚úì Extracted {train_features.shape[1]} minimal features:\")\n",
    "for col in train_features.columns:\n",
    "    print(f\"   ‚Ä¢ {col}\")\n",
    "print(\"\\nNote: These features are for post-hoc analysis, not stacking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation/test split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=SEED, stratify=df[\"label\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"label\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")\n",
    "\n",
    "# Fit feature engineer on training data\n",
    "feature_engineer = FeatureEngineer()\n",
    "feature_engineer.fit(train_df)\n",
    "\n",
    "# Transform all sets\n",
    "train_features = feature_engineer.transform(train_df)\n",
    "val_features = feature_engineer.transform(val_df)\n",
    "test_features = feature_engineer.transform(test_df)\n",
    "\n",
    "print(f\"\\n‚úì Engineered {train_features.shape[1]} features\")\n",
    "print(f\"Features: {list(train_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RoBERTa Training - Clean Loss Function\n",
    "\n",
    "**Why Label-Smoothed CrossEntropy instead of Focal Loss?**\n",
    "\n",
    "| Aspect | Focal Loss (V3.1) | Label-Smoothed CE (V3.2) |\n",
    "|--------|-------------------|--------------------------|\n",
    "| **Class imbalance** | Œ±=0.25 overcorrected | Inverse-frequency weights (data-driven) |\n",
    "| **Confidence calibration** | Œ≥=2 focuses on hard examples | Label smoothing reduces overconfidence |\n",
    "| **Gradient behavior** | Down-weights easy examples | Stable gradients throughout training |\n",
    "| **Empirical result** | 760 FN vs 219 FP (too conservative) | Balanced errors expected |\n",
    "\n",
    "**Loss function:**\n",
    "$$\\mathcal{L} = -\\sum_i w_i \\cdot [(1-\\epsilon) \\cdot y_i \\log(\\hat{y}_i) + \\frac{\\epsilon}{K} \\sum_k \\log(\\hat{y}_{i,k})]$$\n",
    "\n",
    "Where:\n",
    "- $w_i$ = inverse class frequency weight\n",
    "- $\\epsilon$ = 0.1 (label smoothing factor)\n",
    "- $K$ = 2 (number of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WEIGHTED CROSS-ENTROPY WITH LABEL SMOOTHING\n",
    "# =============================================================================\n",
    "#\n",
    "# Statistical justification for class weights:\n",
    "#   - Inverse frequency weighting: w_c = N / (K * n_c)\n",
    "#   - This ensures each class contributes equally to the loss gradient\n",
    "#   - Avoids the arbitrary choice of Focal Loss Œ± parameter\n",
    "#\n",
    "# Label smoothing interpretation:\n",
    "#   - Instead of hard targets [0, 1], use soft targets [Œµ/K, 1-Œµ+Œµ/K]\n",
    "#   - Prevents overconfident predictions\n",
    "#   - Acts as regularization, improving generalization\n",
    "# =============================================================================\n",
    "\n",
    "class WeightedCrossEntropyTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom trainer with:\n",
    "    1. Inverse-frequency class weights (statistically principled)\n",
    "    2. Label smoothing for calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, class_weights=None, label_smoothing=0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Pre-compute smoothed targets\n",
    "        # For label=0: [1-Œµ+Œµ/2, Œµ/2] = [0.95, 0.05]\n",
    "        # For label=1: [Œµ/2, 1-Œµ+Œµ/2] = [0.05, 0.95]\n",
    "        self.smooth_pos = 1.0 - label_smoothing + label_smoothing / 2\n",
    "        self.smooth_neg = label_smoothing / 2\n",
    "        \n",
    "        print(f\"  Class weights: neg={class_weights[0]:.3f}, pos={class_weights[1]:.3f}\")\n",
    "        print(f\"  Label smoothing: {label_smoothing} ‚Üí soft targets [{self.smooth_neg:.2f}, {self.smooth_pos:.2f}]\")\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Create smoothed one-hot targets\n",
    "        batch_size = labels.size(0)\n",
    "        smooth_targets = torch.zeros(batch_size, 2, device=logits.device)\n",
    "        smooth_targets[:, 0] = self.smooth_pos  # Default: label=0\n",
    "        smooth_targets[:, 1] = self.smooth_neg\n",
    "        \n",
    "        # Flip for positive labels\n",
    "        pos_mask = labels == 1\n",
    "        smooth_targets[pos_mask, 0] = self.smooth_neg\n",
    "        smooth_targets[pos_mask, 1] = self.smooth_pos\n",
    "        \n",
    "        # Compute log-softmax\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        # Weighted cross-entropy with smooth targets\n",
    "        # Loss = -sum(w * target * log_prob)\n",
    "        weights = torch.tensor(self.class_weights, device=logits.device)\n",
    "        sample_weights = weights[labels]  # Per-sample weight based on true class\n",
    "        \n",
    "        loss_per_sample = -(smooth_targets * log_probs).sum(dim=-1)\n",
    "        loss = (loss_per_sample * sample_weights).mean()\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Calculate class weights from training data (inverse frequency)\n",
    "n_neg = (train_labels == 0).sum()\n",
    "n_pos = (train_labels == 1).sum()\n",
    "n_total = len(train_labels)\n",
    "n_classes = 2\n",
    "\n",
    "# Inverse frequency: w_c = N / (K * n_c)\n",
    "weight_neg = n_total / (n_classes * n_neg)\n",
    "weight_pos = n_total / (n_classes * n_pos)\n",
    "\n",
    "# Normalize so weights sum to n_classes (standard practice)\n",
    "weight_sum = weight_neg + weight_pos\n",
    "CLASS_WEIGHTS = [weight_neg * n_classes / weight_sum, weight_pos * n_classes / weight_sum]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS WEIGHTING (Inverse Frequency)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Training set: {n_neg:,} negatives, {n_pos:,} positives\")\n",
    "print(f\"  Raw weights: neg={weight_neg:.3f}, pos={weight_pos:.3f}\")\n",
    "print(f\"  Normalized:  neg={CLASS_WEIGHTS[0]:.3f}, pos={CLASS_WEIGHTS[1]:.3f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOKENIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Tokenizing data...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_data(texts, labels):\n",
    "    \"\"\"Tokenize texts for transformer input.\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokens[\"labels\"] = labels.tolist()\n",
    "    return Dataset.from_dict(tokens)\n",
    "\n",
    "train_dataset = tokenize_data(train_df[\"title\"], train_df[\"label\"])\n",
    "val_dataset = tokenize_data(val_df[\"title\"], val_df[\"label\"])\n",
    "test_dataset = tokenize_data(test_df[\"title\"], test_df[\"label\"])\n",
    "\n",
    "# Store labels as numpy for later evaluation\n",
    "train_labels = np.array(train_df[\"label\"])\n",
    "val_labels = np.array(val_df[\"label\"])\n",
    "test_labels = np.array(test_df[\"label\"])\n",
    "\n",
    "print(f\"‚úì Tokenized datasets:\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Val:   {len(val_dataset):,} samples\")\n",
    "print(f\"   Test:  {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROBERTA TRAINING - End-to-End (No Stacking)\n",
    "# =============================================================================\n",
    "#\n",
    "# V3.2 Philosophy: Let the transformer do what it does best.\n",
    "# - No feature engineering in the loop\n",
    "# - No ensemble averaging\n",
    "# - Clean labels + proper loss = optimal learning signal\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ROBERTA (V3.2 - Clean Architecture)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Training arguments with best practices\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_v32\",\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,  # Can use larger batch for eval\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=\"linear\",  # Linear decay after warmup\n",
    "    \n",
    "    # Evaluation & checkpointing\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"roc_auc\",  # Optimize for discrimination, not loss\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Efficiency\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Comprehensive metrics for model selection.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
    "    \n",
    "    # ROC AUC (primary metric for model selection)\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "    \n",
    "    # Precision/Recall at various thresholds\n",
    "    precisions, recalls, thresholds = precision_recall_curve(labels, probs)\n",
    "    \n",
    "    # Find F1-optimal threshold\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 0.5\n",
    "    \n",
    "    # Metrics at 0.5 threshold (standard)\n",
    "    preds_05 = (probs >= 0.5).astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"avg_precision\": average_precision_score(labels, probs),\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"accuracy_05\": accuracy_score(labels, preds_05),\n",
    "        \"precision_05\": precision_score(labels, preds_05, zero_division=0),\n",
    "        \"recall_05\": recall_score(labels, preds_05, zero_division=0),\n",
    "    }\n",
    "\n",
    "# Initialize trainer with weighted loss\n",
    "trainer = WeightedCrossEntropyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    class_weights=CLASS_WEIGHTS,\n",
    "    label_smoothing=LABEL_SMOOTHING,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(\"\\nStarting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total steps: {train_result.global_step}\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXTRACT PREDICTIONS - Raw probabilities from RoBERTa\n",
    "# =============================================================================\n",
    "\n",
    "def get_predictions(dataset, batch_size=64, desc=\"Extracting predictions\"):\n",
    "    \"\"\"Get probability predictions from the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    # Prepare dataset without labels for inference\n",
    "    dataset_no_labels = dataset.remove_columns([\"labels\"])\n",
    "    dataset_no_labels.set_format(\"torch\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset_no_labels), batch_size), desc=desc):\n",
    "            batch = dataset_no_labels[i:i+batch_size]\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "# Get predictions for all splits\n",
    "print(\"Extracting predictions from trained model...\")\n",
    "train_probs_raw = get_predictions(train_dataset, desc=\"Train set\")\n",
    "val_probs_raw = get_predictions(val_dataset, desc=\"Val set\")\n",
    "test_probs_raw = get_predictions(test_dataset, desc=\"Test set\")\n",
    "\n",
    "# Evaluate raw model performance\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RAW MODEL PERFORMANCE (before calibration)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Train ROC AUC: {roc_auc_score(train_labels, train_probs_raw):.4f}\")\n",
    "print(f\"  Val ROC AUC:   {roc_auc_score(val_labels, val_probs_raw):.4f}\")\n",
    "print(f\"  Test ROC AUC:  {roc_auc_score(test_labels, test_probs_raw):.4f}\")\n",
    "print(f\"  Test Avg Precision: {average_precision_score(test_labels, test_probs_raw):.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Isotonic Calibration (Preserved from V3.1)\n",
    "\n",
    "Isotonic regression provides non-parametric probability calibration:\n",
    "- Maps raw model outputs to well-calibrated probabilities\n",
    "- Preserves ranking (monotonic transformation)\n",
    "- Achieved ECE = 0.01 in V3.1 (excellent)\n",
    "\n",
    "Unlike temperature scaling (parametric), isotonic regression can correct non-linear miscalibration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ISOTONIC CALIBRATION\n",
    "# =============================================================================\n",
    "#\n",
    "# Why isotonic regression?\n",
    "# - Non-parametric: No assumptions about the form of miscalibration\n",
    "# - Monotonic: Preserves model ranking (ROC AUC unchanged)\n",
    "# - Proven: ECE = 0.01 in V3.1\n",
    "#\n",
    "# We fit on validation set to avoid overfitting to test.\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_ece(probs, labels, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate Expected Calibration Error.\n",
    "    \n",
    "    ECE = Œ£ (|bin_count| / N) * |accuracy(bin) - confidence(bin)|\n",
    "    \n",
    "    Lower is better. ECE < 0.05 is considered well-calibrated.\n",
    "    \"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        in_bin = (probs > bin_boundaries[i]) & (probs <= bin_boundaries[i+1])\n",
    "        if in_bin.sum() > 0:\n",
    "            bin_accuracy = labels[in_bin].mean()\n",
    "            bin_confidence = probs[in_bin].mean()\n",
    "            ece += (in_bin.sum() / len(probs)) * abs(bin_accuracy - bin_confidence)\n",
    "    \n",
    "    return ece\n",
    "\n",
    "print(\"Applying Isotonic Regression calibration...\")\n",
    "\n",
    "# Fit calibrator on validation set\n",
    "calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "calibrator.fit(val_probs_raw, val_labels)\n",
    "\n",
    "# Apply to all sets\n",
    "train_probs = calibrator.predict(train_probs_raw)\n",
    "val_probs = calibrator.predict(val_probs_raw)\n",
    "test_probs = calibrator.predict(test_probs_raw)\n",
    "\n",
    "# Measure calibration improvement\n",
    "ece_before = calculate_ece(test_probs_raw, test_labels)\n",
    "ece_after = calculate_ece(test_probs, test_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CALIBRATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  ECE before calibration: {ece_before:.4f}\")\n",
    "print(f\"  ECE after calibration:  {ece_after:.4f}\")\n",
    "if ece_before > 0:\n",
    "    print(f\"  Improvement: {((ece_before - ece_after) / ece_before * 100):.1f}%\")\n",
    "print(f\"  Well-calibrated: {'‚úì Yes' if ece_after < 0.05 else '‚úó No'} (threshold: 0.05)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold Optimization & Evaluation\n",
    "\n",
    "**Threshold Selection Strategies:**\n",
    "1. **F1-Optimal**: Maximizes F1 score (harmonic mean of precision/recall)\n",
    "2. **Balanced P/R**: Where precision ‚âà recall\n",
    "3. **High-Precision**: For use cases that can't tolerate false positives\n",
    "\n",
    "The optimal threshold depends on your use case:\n",
    "- RSS reader highlighting ‚Üí probably want higher precision (fewer false alerts)\n",
    "- Content discovery ‚Üí balanced or high recall (don't miss good content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# THRESHOLD OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Analyzing classification thresholds...\")\n",
    "\n",
    "# Get precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(test_labels, test_probs)\n",
    "\n",
    "# Strategy 1: F1-Optimal threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "f1_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 0.5\n",
    "\n",
    "# Strategy 2: Balanced P/R threshold\n",
    "balanced_diff = np.abs(precisions[:-1] - recalls[:-1])\n",
    "balanced_idx = np.argmin(balanced_diff)\n",
    "balanced_threshold = thresholds[balanced_idx]\n",
    "\n",
    "# Strategy 3: High-precision thresholds\n",
    "def find_precision_threshold(target_precision, min_recall=0.2):\n",
    "    \"\"\"Find threshold achieving target precision with minimum recall.\"\"\"\n",
    "    valid = recalls[:-1] >= min_recall\n",
    "    diffs = np.abs(precisions[:-1] - target_precision)\n",
    "    diffs[~valid] = np.inf\n",
    "    if np.all(np.isinf(diffs)):\n",
    "        return 0.5\n",
    "    idx = np.argmin(diffs)\n",
    "    return thresholds[idx]\n",
    "\n",
    "precision_60_threshold = find_precision_threshold(0.60)\n",
    "precision_70_threshold = find_precision_threshold(0.70)\n",
    "precision_80_threshold = find_precision_threshold(0.80, min_recall=0.15)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"THRESHOLD ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Strategy':<25} {'Threshold':>10} {'Precision':>10} {'Recall':>10} {'F1':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "thresholds_to_eval = [\n",
    "    (\"F1-Optimal\", f1_threshold),\n",
    "    (\"Balanced P/R\", balanced_threshold),\n",
    "    (\"Precision ~60%\", precision_60_threshold),\n",
    "    (\"Precision ~70%\", precision_70_threshold),\n",
    "    (\"Precision ~80%\", precision_80_threshold),\n",
    "    (\"Standard (0.5)\", 0.5),\n",
    "]\n",
    "\n",
    "for name, thresh in thresholds_to_eval:\n",
    "    preds = (test_probs >= thresh).astype(int)\n",
    "    p = precision_score(test_labels, preds, zero_division=0)\n",
    "    r = recall_score(test_labels, preds, zero_division=0)\n",
    "    f1 = f1_score(test_labels, preds, zero_division=0)\n",
    "    print(f\"{name:<25} {thresh:>10.3f} {p:>10.3f} {r:>10.3f} {f1:>10.3f}\")\n",
    "\n",
    "# Use F1-optimal as default (can be changed based on use case)\n",
    "OPTIMAL_THRESHOLD = f1_threshold\n",
    "print(f\"\\n‚úì Selected default threshold: {OPTIMAL_THRESHOLD:.3f} (F1-optimal)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. ROC Curve with version comparison\n",
    "ax1 = axes[0, 0]\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_probs)\n",
    "ax1.plot(fpr, tpr, \"b-\", linewidth=2, label=f\"V3.2 (AUC={roc_auc:.3f})\")\n",
    "ax1.plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Random\")\n",
    "ax1.fill_between(fpr, tpr, alpha=0.2)\n",
    "\n",
    "# Add reference points for previous versions\n",
    "ax1.scatter([0.33], [0.77], color=\"green\", s=100, marker=\"^\", zorder=5, label=f\"V1 (AUC=0.770)\")\n",
    "ax1.scatter([0.30], [0.72], color=\"orange\", s=100, marker=\"s\", zorder=5, label=f\"V3.1 (AUC=0.715)\")\n",
    "\n",
    "ax1.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "ax1.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "ax1.set_title(\"ROC Curve - Version Comparison\", fontsize=14)\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve - FIX: slice both arrays consistently\n",
    "ax2 = axes[0, 1]\n",
    "# precision_recall_curve returns: precisions (n+1,), recalls (n+1,), thresholds (n,)\n",
    "# We need to plot precisions[:-1] vs recalls[:-1] to match thresholds\n",
    "ax2.plot(recalls[:-1], precisions[:-1], \"g-\", linewidth=2, label=f\"V3.2 (AP={avg_precision:.3f})\")\n",
    "ax2.axhline(y=test_labels.mean(), color=\"r\", linestyle=\"--\", alpha=0.5, \n",
    "            label=f\"Random (AP={test_labels.mean():.3f})\")\n",
    "ax2.scatter([recall], [precision], color=\"red\", s=150, zorder=5, \n",
    "            label=f\"Operating point (P={precision:.2f}, R={recall:.2f})\")\n",
    "ax2.set_xlabel(\"Recall\", fontsize=12)\n",
    "ax2.set_ylabel(\"Precision\", fontsize=12)\n",
    "ax2.set_title(\"Precision-Recall Curve\", fontsize=14)\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "# 3. Calibration Plot\n",
    "ax3 = axes[1, 0]\n",
    "n_bins = 10\n",
    "bin_means, bin_true, bin_counts = [], [], []\n",
    "for i in range(n_bins):\n",
    "    low, high = i / n_bins, (i + 1) / n_bins\n",
    "    mask = (test_probs >= low) & (test_probs < high)\n",
    "    if mask.sum() > 0:\n",
    "        bin_means.append(test_probs[mask].mean())\n",
    "        bin_true.append(test_labels[mask].mean())\n",
    "        bin_counts.append(mask.sum())\n",
    "\n",
    "ax3.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\", linewidth=2)\n",
    "scatter = ax3.scatter(bin_means, bin_true, s=[c/10 for c in bin_counts], alpha=0.7, \n",
    "                      c=bin_counts, cmap=\"Blues\", label=\"V3.2 Calibrated\")\n",
    "ax3.plot(bin_means, bin_true, \"b-\", alpha=0.5)\n",
    "ax3.set_xlabel(\"Mean Predicted Probability\", fontsize=12)\n",
    "ax3.set_ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "ax3.set_title(f\"Calibration Plot (ECE={ece_after:.4f})\", fontsize=14)\n",
    "ax3.legend(loc=\"upper left\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax3, label=\"Bin count\")\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "ax4 = axes[1, 1]\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\",\", cmap=\"Blues\", ax=ax4,\n",
    "            xticklabels=[\"Predicted Miss\", \"Predicted Hit\"],\n",
    "            yticklabels=[\"Actual Miss\", \"Actual Hit\"],\n",
    "            annot_kws={\"size\": 14})\n",
    "ax4.set_title(f\"Confusion Matrix (threshold={OPTIMAL_THRESHOLD:.2f})\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"v32_evaluation_plots.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"\\n‚úì Plots saved to v32_evaluation_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all model components\n",
    "print(\"Saving model components...\")\n",
    "os.makedirs(\"./hn_model_v32\", exist_ok=True)\n",
    "\n",
    "# Save RoBERTa model\n",
    "model.save_pretrained(\"./hn_model_v32/roberta\")\n",
    "tokenizer.save_pretrained(\"./hn_model_v32/roberta\")\n",
    "print(\"‚úì RoBERTa model saved\")\n",
    "\n",
    "# Save calibrator\n",
    "joblib.dump(calibrator, \"./hn_model_v32/isotonic_calibrator.joblib\")\n",
    "print(\"‚úì Isotonic calibrator saved\")\n",
    "\n",
    "# Save feature engineer\n",
    "joblib.dump(feature_engineer, \"./hn_model_v32/feature_engineer.joblib\")\n",
    "print(\"‚úì Feature engineer saved\")\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    \"version\": \"v3.2\",\n",
    "    \"architecture\": \"Pure RoBERTa + Label-Smoothed CE + Isotonic Calibration\",\n",
    "    \"description\": \"Clean architecture - removed TF-IDF and LightGBM stacking\",\n",
    "    \"optimal_threshold\": float(OPTIMAL_THRESHOLD),\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "    \"metrics\": {\n",
    "        \"roc_auc\": float(roc_auc),\n",
    "        \"average_precision\": float(avg_precision),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"ece\": float(ece_after)\n",
    "    },\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "    \"training_config\": {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"hit_threshold\": HIT_THRESHOLD\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"./hn_model_v32/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"‚úì Configuration saved\")\n",
    "\n",
    "# Create zip for download\n",
    "!cd hn_model_v32 && zip -r ../hn_model_v32.zip .\n",
    "print(\"\\n‚úì All components saved to hn_model_v32.zip\")\n",
    "\n",
    "# List files\n",
    "!ls -la hn_model_v32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model zip\n",
    "from google.colab import files\n",
    "files.download(\"hn_model_v32.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
