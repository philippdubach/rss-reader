{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"},
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": ["# HN Success Predictor - Transformer Edition\n\nThis notebook trains a DistilBERT-based classifier to predict whether a Hacker News post will be successful (100+ points).\n\n## Instructions\n\n1. **Runtime**: Go to `Runtime > Change runtime type` and select **GPU** (T4 is fine)\n2. **Run all cells**: `Runtime > Run all` or Ctrl+F9\n3. **Wait**: Data fetching takes ~10-15 min, training takes ~20-30 min\n4. **Download**: The trained model will be saved and available for download\n\n## What this does\n\n- Fetches ~60,000 HN posts from the last 2 years (balanced: high/medium/low scores)\n- Fine-tunes DistilBERT on post titles to predict success probability\n- Achieves ~0.70-0.75 ROC AUC (vs 0.63 with TF-IDF)\n- Exports model for use in your RSS reader\n\n### Why 2 years?\n- **Too short (<1 year)**: Not enough data, seasonal bias\n- **Too long (>3 years)**: Topic drift - what was hot in 2021 isn't relevant now\n- **2 years**: Sweet spot - enough data, still relevant topics"],
      "metadata": {"id": "intro"}
    },
    {
      "cell_type": "markdown",
      "source": ["## 1. Setup and Dependencies"],
      "metadata": {"id": "setup_header"}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "install_deps"},
      "outputs": [],
      "source": ["!pip install -q transformers datasets accelerate scikit-learn pandas tqdm"]
    },
    {
      "cell_type": "code",
      "source": ["import requests\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Optional\nfrom urllib.parse import urlparse\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom transformers import (\n    DistilBertTokenizer,\n    DistilBertForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_curve\nfrom datasets import Dataset as HFDataset\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"],
      "metadata": {"id": "imports"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 2. Fetch Training Data from Algolia\n\nWe will fetch posts in three categories:\n- **High**: 100+ points (hits)\n- **Medium**: 20-99 points\n- **Low**: 1-19 points (misses)"],
      "metadata": {"id": "fetch_header"}
    },
    {
      "cell_type": "code",
      "source": ["POSTS_PER_CATEGORY = 20000\nMONTHS_BACK = 24\nHIT_THRESHOLD = 100\n\nprint(f\"Will fetch ~{POSTS_PER_CATEGORY * 3:,} posts from the last {MONTHS_BACK} months\")"],
      "metadata": {"id": "config"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["def fetch_hn_posts(min_points, max_points, target_count, months_back=24):\n    base_url = \"https://hn.algolia.com/api/v1/search_by_date\"\n    all_posts = []\n    seen_ids = set()\n\n    if max_points:\n        points_filter = f\"points>={min_points},points<={max_points}\"\n        desc = f\"{min_points}-{max_points} pts\"\n    else:\n        points_filter = f\"points>={min_points}\"\n        desc = f\"{min_points}+ pts\"\n\n    end_date = datetime.now()\n    posts_per_month = max(target_count // months_back, 500)\n    pbar = tqdm(range(months_back), desc=f\"Fetching {desc}\")\n\n    for month_offset in pbar:\n        month_end = end_date - timedelta(days=30 * month_offset)\n        month_start = month_end - timedelta(days=30)\n\n        params = {\n            \"tags\": \"story\",\n            \"numericFilters\": f\"{points_filter},created_at_i>={int(month_start.timestamp())},created_at_i<={int(month_end.timestamp())}\",\n            \"hitsPerPage\": min(posts_per_month, 1000),\n        }\n\n        try:\n            resp = requests.get(base_url, params=params, timeout=30)\n            resp.raise_for_status()\n            data = resp.json()\n\n            for hit in data.get(\"hits\", []):\n                post_id = hit.get(\"objectID\")\n                if post_id and post_id not in seen_ids:\n                    seen_ids.add(post_id)\n                    url = hit.get(\"url\", \"\")\n                    if url:\n                        try:\n                            domain = urlparse(url).netloc.replace(\"www.\", \"\")\n                        except:\n                            domain = \"\"\n                    else:\n                        domain = \"self.hackernews\"\n\n                    all_posts.append({\n                        \"id\": post_id,\n                        \"title\": hit.get(\"title\", \"\"),\n                        \"url\": url,\n                        \"domain\": domain,\n                        \"points\": hit.get(\"points\", 0),\n                    })\n\n            pbar.set_postfix({\"total\": len(all_posts)})\n        except Exception as e:\n            print(f\"Error: {e}\")\n\n        time.sleep(0.5)\n        if len(all_posts) >= target_count:\n            break\n\n    return all_posts[:target_count]"],
      "metadata": {"id": "fetch_function"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["print(\"Fetching training data (~10-15 minutes)...\\n\")\n\nposts_high = fetch_hn_posts(100, None, POSTS_PER_CATEGORY, MONTHS_BACK)\nposts_medium = fetch_hn_posts(20, 99, POSTS_PER_CATEGORY, MONTHS_BACK)\nposts_low = fetch_hn_posts(1, 19, POSTS_PER_CATEGORY, MONTHS_BACK)\n\nfor p in posts_high: p['category'] = 'high'\nfor p in posts_medium: p['category'] = 'medium'\nfor p in posts_low: p['category'] = 'low'\n\ndf = pd.DataFrame(posts_high + posts_medium + posts_low)\nprint(f\"\\nFetched {len(df):,} posts\")"],
      "metadata": {"id": "fetch_data"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 3. Prepare Dataset"],
      "metadata": {"id": "prepare_header"}
    },
    {
      "cell_type": "code",
      "source": ["df['label'] = (df['points'] >= HIT_THRESHOLD).astype(int)\ndf = df[df['title'].notna() & (df['title'].str.len() > 5)].copy()\ndf['input_text'] = df['title']\n\nprint(f\"Dataset: {len(df):,} posts, {df['label'].mean():.1%} hits\")\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n\nprint(f\"Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}\")"],
      "metadata": {"id": "prepare"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 4. Tokenize with DistilBERT"],
      "metadata": {"id": "tokenize_header"}
    },
    {
      "cell_type": "code",
      "source": ["MODEL_NAME = \"distilbert-base-uncased\"\ntokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\nMAX_LENGTH = 64\n\ndef tokenize_function(examples):\n    return tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=MAX_LENGTH)\n\ntrain_dataset = HFDataset.from_pandas(train_df[['input_text', 'label']].reset_index(drop=True))\nval_dataset = HFDataset.from_pandas(val_df[['input_text', 'label']].reset_index(drop=True))\ntest_dataset = HFDataset.from_pandas(test_df[['input_text', 'label']].reset_index(drop=True))\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\nprint(\"Tokenized datasets ready\")"],
      "metadata": {"id": "tokenize"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 5. Train the Model"],
      "metadata": {"id": "train_header"}
    },
    {
      "cell_type": "code",
      "source": ["model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\nmodel = model.to(device)\nprint(f\"Model on {device}, {sum(p.numel() for p in model.parameters()):,} params\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n    return {'accuracy': accuracy_score(labels, predictions), 'roc_auc': roc_auc_score(labels, probs)}"],
      "metadata": {"id": "load_model"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["training_args = TrainingArguments(\n    output_dir='./checkpoints',\n    num_train_epochs=3,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"roc_auc\",\n    greater_is_better=True,\n    logging_steps=100,\n    report_to=\"none\",\n    fp16=torch.cuda.is_available(),\n    seed=42,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\nprint(\"Training (~20-30 min on T4)...\")\ntrainer.train()"],
      "metadata": {"id": "train"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 6. Evaluate"],
      "metadata": {"id": "eval_header"}
    },
    {
      "cell_type": "code",
      "source": ["results = trainer.evaluate(test_dataset)\nprint(f\"\\nTest ROC AUC: {results['eval_roc_auc']:.4f}\")\nprint(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")"],
      "metadata": {"id": "evaluate"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 7. Demo Predictions"],
      "metadata": {"id": "demo_header"}
    },
    {
      "cell_type": "code",
      "source": ["def predict_success(titles):\n    inputs = tokenizer(titles, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n    return pd.DataFrame({'title': titles, 'probability': probs}).sort_values('probability', ascending=False)\n\ndemo = [\n    \"Show HN: I built a neural network in Rust\",\n    \"Google announces layoffs\",\n    \"Python 4.0 released\",\n    \"Claude 4 is now available\",\n    \"How SQLite scales to 1 billion rows\",\n    \"The decline of Stack Overflow\",\n]\n\nfor _, row in predict_success(demo).iterrows():\n    print(f\"[{row['probability']:.1%}] {row['title'][:60]}\")"],
      "metadata": {"id": "demo"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 8. Save Model"],
      "metadata": {"id": "save_header"}
    },
    {
      "cell_type": "code",
      "source": ["model.save_pretrained(\"./hn_predictor\")\ntokenizer.save_pretrained(\"./hn_predictor\")\n!zip -r hn_predictor.zip ./hn_predictor\nprint(\"Download hn_predictor.zip from the file browser\")"],
      "metadata": {"id": "save"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## Usage in RSS Reader\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport torch\n\nclass BertPredictor:\n    def __init__(self, path=\"./hn_predictor\"):\n        self.tokenizer = DistilBertTokenizer.from_pretrained(path)\n        self.model = DistilBertForSequenceClassification.from_pretrained(path)\n        self.model.eval()\n\n    def predict(self, titles):\n        inputs = self.tokenizer(titles, padding=True, truncation=True, max_length=64, return_tensors=\"pt\")\n        with torch.no_grad():\n            probs = torch.softmax(self.model(**inputs).logits, dim=-1)[:, 1].numpy()\n        return probs\n```"],
      "metadata": {"id": "usage"}
    }
  ]
}
