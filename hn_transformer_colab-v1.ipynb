{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# HN Success Predictor - Transformer Edition\n",
        "\n",
        "This notebook trains a DistilBERT-based classifier to predict whether a Hacker News post will be successful (100+ points).\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Runtime**: Go to `Runtime > Change runtime type` and select **GPU** (T4 is fine)\n",
        "2. **Run all cells**: `Runtime > Run all` or Ctrl+F9\n",
        "3. **Wait**: Data fetching takes ~10-15 min, training takes ~20-30 min\n",
        "4. **Download**: The trained model will be saved and available for download\n",
        "\n",
        "## What this does\n",
        "\n",
        "- Fetches ~60,000 HN posts from the last 2 years (balanced: high/medium/low scores)\n",
        "- Fine-tunes DistilBERT on post titles to predict success probability\n",
        "- Achieves ~0.70-0.75 ROC AUC (vs 0.63 with TF-IDF)\n",
        "- Exports model for use in your RSS reader\n",
        "\n",
        "### Why 2 years?\n",
        "- **Too short (<1 year)**: Not enough data, seasonal bias\n",
        "- **Too long (>3 years)**: Topic drift - what was hot in 2021 isn't relevant now\n",
        "- **2 years**: Sweet spot - enough data, still relevant topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate scikit-learn pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_curve\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fetch_header"
      },
      "source": [
        "## 2. Fetch Training Data from Algolia\n",
        "\n",
        "We will fetch posts in three categories:\n",
        "- **High**: 100+ points (hits)\n",
        "- **Medium**: 20-99 points\n",
        "- **Low**: 1-19 points (misses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "POSTS_PER_CATEGORY = 20000\n",
        "MONTHS_BACK = 24\n",
        "HIT_THRESHOLD = 100\n",
        "\n",
        "print(f\"Will fetch ~{POSTS_PER_CATEGORY * 3:,} posts from the last {MONTHS_BACK} months\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fetch_function"
      },
      "outputs": [],
      "source": [
        "def fetch_hn_posts(min_points, max_points, target_count, months_back=24):\n",
        "    base_url = \"https://hn.algolia.com/api/v1/search_by_date\"\n",
        "    all_posts = []\n",
        "    seen_ids = set()\n",
        "\n",
        "    if max_points:\n",
        "        points_filter = f\"points>={min_points},points<={max_points}\"\n",
        "        desc = f\"{min_points}-{max_points} pts\"\n",
        "    else:\n",
        "        points_filter = f\"points>={min_points}\"\n",
        "        desc = f\"{min_points}+ pts\"\n",
        "\n",
        "    end_date = datetime.now()\n",
        "    posts_per_month = max(target_count // months_back, 500)\n",
        "    pbar = tqdm(range(months_back), desc=f\"Fetching {desc}\")\n",
        "\n",
        "    for month_offset in pbar:\n",
        "        month_end = end_date - timedelta(days=30 * month_offset)\n",
        "        month_start = month_end - timedelta(days=30)\n",
        "\n",
        "        params = {\n",
        "            \"tags\": \"story\",\n",
        "            \"numericFilters\": f\"{points_filter},created_at_i>={int(month_start.timestamp())},created_at_i<={int(month_end.timestamp())}\",\n",
        "            \"hitsPerPage\": min(posts_per_month, 1000),\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            resp = requests.get(base_url, params=params, timeout=30)\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json()\n",
        "\n",
        "            for hit in data.get(\"hits\", []):\n",
        "                post_id = hit.get(\"objectID\")\n",
        "                if post_id and post_id not in seen_ids:\n",
        "                    seen_ids.add(post_id)\n",
        "                    url = hit.get(\"url\", \"\")\n",
        "                    if url:\n",
        "                        try:\n",
        "                            domain = urlparse(url).netloc.replace(\"www.\", \"\")\n",
        "                        except:\n",
        "                            domain = \"\"\n",
        "                    else:\n",
        "                        domain = \"self.hackernews\"\n",
        "\n",
        "                    all_posts.append({\n",
        "                        \"id\": post_id,\n",
        "                        \"title\": hit.get(\"title\", \"\"),\n",
        "                        \"url\": url,\n",
        "                        \"domain\": domain,\n",
        "                        \"points\": hit.get(\"points\", 0),\n",
        "                    })\n",
        "\n",
        "            pbar.set_postfix({\"total\": len(all_posts)})\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "        time.sleep(0.5)\n",
        "        if len(all_posts) >= target_count:\n",
        "            break\n",
        "\n",
        "    return all_posts[:target_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fetch_data"
      },
      "outputs": [],
      "source": [
        "print(\"Fetching training data (~10-15 minutes)...\\n\")\n",
        "\n",
        "posts_high = fetch_hn_posts(100, None, POSTS_PER_CATEGORY, MONTHS_BACK)\n",
        "posts_medium = fetch_hn_posts(20, 99, POSTS_PER_CATEGORY, MONTHS_BACK)\n",
        "posts_low = fetch_hn_posts(1, 19, POSTS_PER_CATEGORY, MONTHS_BACK)\n",
        "\n",
        "for p in posts_high: p['category'] = 'high'\n",
        "for p in posts_medium: p['category'] = 'medium'\n",
        "for p in posts_low: p['category'] = 'low'\n",
        "\n",
        "df = pd.DataFrame(posts_high + posts_medium + posts_low)\n",
        "print(f\"\\nFetched {len(df):,} posts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_header"
      },
      "source": [
        "## 3. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare"
      },
      "outputs": [],
      "source": [
        "df['label'] = (df['points'] >= HIT_THRESHOLD).astype(int)\n",
        "df = df[df['title'].notna() & (df['title'].str.len() > 5)].copy()\n",
        "df['input_text'] = df['title']\n",
        "\n",
        "print(f\"Dataset: {len(df):,} posts, {df['label'].mean():.1%} hits\")\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
        "\n",
        "print(f\"Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenize_header"
      },
      "source": [
        "## 4. Tokenize with DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokenize"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "MAX_LENGTH = 64\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "train_dataset = HFDataset.from_pandas(train_df[['input_text', 'label']].reset_index(drop=True))\n",
        "val_dataset = HFDataset.from_pandas(val_df[['input_text', 'label']].reset_index(drop=True))\n",
        "test_dataset = HFDataset.from_pandas(test_df[['input_text', 'label']].reset_index(drop=True))\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Tokenized datasets ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_header"
      },
      "source": [
        "## 5. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "model = model.to(device)\n",
        "print(f\"Model on {device}, {sum(p.numel() for p in model.parameters()):,} params\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
        "    return {'accuracy': accuracy_score(labels, predictions), 'roc_auc': roc_auc_score(labels, probs)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./checkpoints',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"roc_auc\",\n",
        "    greater_is_better=True,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "print(\"Training (~20-30 min on T4)...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_header"
      },
      "source": [
        "## 6. Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate(test_dataset)\n",
        "print(f\"\\nTest ROC AUC: {results['eval_roc_auc']:.4f}\")\n",
        "print(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_header"
      },
      "source": [
        "## 7. Demo Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo"
      },
      "outputs": [],
      "source": [
        "def predict_success(titles):\n",
        "    inputs = tokenizer(titles, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
        "    return pd.DataFrame({'title': titles, 'probability': probs}).sort_values('probability', ascending=False)\n",
        "\n",
        "demo = [\n",
        "    \"Show HN: I built a neural network in Rust\",\n",
        "    \"Google announces layoffs\",\n",
        "    \"Python 4.0 released\",\n",
        "    \"Claude 4 is now available\",\n",
        "    \"How SQLite scales to 1 billion rows\",\n",
        "    \"The decline of Stack Overflow\",\n",
        "]\n",
        "\n",
        "for _, row in predict_success(demo).iterrows():\n",
        "    print(f\"[{row['probability']:.1%}] {row['title'][:60]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## 8. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./hn_predictor\")\n",
        "tokenizer.save_pretrained(\"./hn_predictor\")\n",
        "!zip -r hn_predictor.zip ./hn_predictor\n",
        "print(\"Download hn_predictor.zip from the file browser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage"
      },
      "source": [
        "## Usage in RSS Reader\n",
        "\n",
        "```python\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "class BertPredictor:\n",
        "    def __init__(self, path=\"./hn_predictor\"):\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(path)\n",
        "        self.model = DistilBertForSequenceClassification.from_pretrained(path)\n",
        "        self.model.eval()\n",
        "\n",
        "    def predict(self, titles):\n",
        "        inputs = self.tokenizer(titles, padding=True, truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(self.model(**inputs).logits, dim=-1)[:, 1].numpy()\n",
        "        return probs\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
