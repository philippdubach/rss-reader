{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HN Success Predictor V3 - Stacking Meta-Learner\n",
    "\n",
    "**Architecture:** RoBERTa (Focal Loss) + TF-IDF + Engineered Features â†’ LightGBM Stacker â†’ Isotonic Calibration\n",
    "\n",
    "**Key Improvements:**\n",
    "- Focal Loss (Î±=0.25, Î³=2.0) to reduce false positives\n",
    "- LightGBM meta-learner instead of weighted average\n",
    "- Isotonic calibration for probability estimates\n",
    "- 30+ engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate scikit-learn pandas tqdm lightgbm seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, precision_recall_curve, roc_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from datasets import Dataset\n",
    "import lightgbm as lgb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "SEED = 42\n",
    "\n",
    "# Focal Loss parameters (key for reducing FP)\n",
    "FOCAL_ALPHA = 0.25  # Weight for positive class (lower = penalize FP more)\n",
    "FOCAL_GAMMA = 2.0   # Focus on hard examples\n",
    "\n",
    "# LightGBM stacker parameters\n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'is_unbalance': True,\n",
    "    'verbose': -1,\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "# Data settings\n",
    "POSTS_PER_CATEGORY = 30000\n",
    "MIN_SCORE_HIT = 100\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print(\"âœ“ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_hn_posts(min_points, max_points, target_count, months_back=24):\n",
    "    \"\"\"Fetch HN posts from Algolia API with time-based pagination.\"\"\"\n",
    "    base_url = \"https://hn.algolia.com/api/v1/search_by_date\"\n",
    "    all_posts = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    if max_points:\n",
    "        points_filter = f\"points>={min_points},points<={max_points}\"\n",
    "        desc = f\"{min_points}-{max_points} pts\"\n",
    "    else:\n",
    "        points_filter = f\"points>={min_points}\"\n",
    "        desc = f\"{min_points}+ pts\"\n",
    "\n",
    "    end_date = datetime.now()\n",
    "    pbar = tqdm(range(months_back), desc=f\"Fetching {desc}\")\n",
    "\n",
    "    for month_offset in pbar:\n",
    "        month_end = end_date - timedelta(days=30 * month_offset)\n",
    "        month_start = month_end - timedelta(days=30)\n",
    "        \n",
    "        # Paginate within each month (up to 10 pages of 1000)\n",
    "        for page in range(10):\n",
    "            if len(all_posts) >= target_count:\n",
    "                break\n",
    "                \n",
    "            params = {\n",
    "                \"tags\": \"story\",\n",
    "                \"numericFilters\": f\"{points_filter},created_at_i>={int(month_start.timestamp())},created_at_i<={int(month_end.timestamp())}\",\n",
    "                \"hitsPerPage\": 1000,\n",
    "                \"page\": page,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(base_url, params=params, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                data = resp.json()\n",
    "                hits = data.get(\"hits\", [])\n",
    "                \n",
    "                if not hits:\n",
    "                    break\n",
    "\n",
    "                for hit in hits:\n",
    "                    post_id = hit.get(\"objectID\")\n",
    "                    if post_id and post_id not in seen_ids:\n",
    "                        seen_ids.add(post_id)\n",
    "                        url = hit.get(\"url\", \"\")\n",
    "                        domain = \"\"\n",
    "                        if url:\n",
    "                            try:\n",
    "                                domain = urlparse(url).netloc.replace(\"www.\", \"\")\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                        all_posts.append({\n",
    "                            \"title\": hit.get(\"title\", \"\"),\n",
    "                            \"url\": url,\n",
    "                            \"domain\": domain,\n",
    "                            \"score\": hit.get(\"points\", 0),\n",
    "                            \"submitter\": hit.get(\"author\", \"\"),\n",
    "                            \"timestamp\": hit.get(\"created_at\", \"\"),\n",
    "                            \"num_comments\": hit.get(\"num_comments\", 0),\n",
    "                        })\n",
    "\n",
    "                pbar.set_postfix({\"total\": len(all_posts)})\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                break\n",
    "\n",
    "        if len(all_posts) >= target_count:\n",
    "            break\n",
    "\n",
    "    return all_posts[:target_count]\n",
    "\n",
    "print(\"âœ“ Data fetching function defined (time-based pagination)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch posts from each category\n",
    "print(\"Fetching HN posts...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Hits: posts with score >= 100\n",
    "print(f\"Fetching {POSTS_PER_CATEGORY:,} hit posts (score >= {MIN_SCORE_HIT})...\")\n",
    "hits = fetch_hn_posts(min_points=MIN_SCORE_HIT, max_points=None, target_count=POSTS_PER_CATEGORY, months_back=24)\n",
    "print(f\"  Got {len(hits):,} hit posts\")\n",
    "\n",
    "# Misses: posts with score 0-9\n",
    "print(f\"Fetching {POSTS_PER_CATEGORY:,} miss posts (score 0-9)...\")\n",
    "misses = fetch_hn_posts(min_points=0, max_points=9, target_count=POSTS_PER_CATEGORY, months_back=24)\n",
    "print(f\"  Got {len(misses):,} miss posts\")\n",
    "\n",
    "# Medium: posts with score 10-50 (for calibration diversity)\n",
    "print(f\"Fetching {POSTS_PER_CATEGORY:,} medium posts (score 10-50)...\")\n",
    "medium_posts = fetch_hn_posts(min_points=10, max_points=50, target_count=POSTS_PER_CATEGORY, months_back=24)\n",
    "print(f\"  Got {len(medium_posts):,} medium posts\")\n",
    "\n",
    "# Combine and label\n",
    "for p in hits:\n",
    "    p[\"label\"] = 1\n",
    "for p in misses:\n",
    "    p[\"label\"] = 0\n",
    "for p in medium_posts:\n",
    "    p[\"label\"] = 1 if p[\"score\"] >= 30 else 0\n",
    "\n",
    "all_posts = hits + misses + medium_posts\n",
    "df = pd.DataFrame(all_posts)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "print(f\"\\nâœ“ Total posts: {len(df):,}\")\n",
    "print(f\"  Hits (label=1): {(df['label']==1).sum():,}\")\n",
    "print(f\"  Misses (label=0): {(df['label']==0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Extract features for the stacking meta-learner.\n",
    "    \n",
    "    NOTE: Domain/author hit rates removed - they cause data leakage and \n",
    "    reduce ROC AUC by making the model ignore transformer semantics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_hit_rates = {}\n",
    "        self.global_prior = 0.5\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Learn statistics from training data.\"\"\"\n",
    "        # Global prior\n",
    "        self.global_prior = df[\"label\"].mean()\n",
    "        \n",
    "        # Word-level hit rates (less leaky - aggregates over many posts)\n",
    "        hit_titles = df[df[\"label\"] == 1][\"title\"].str.lower()\n",
    "        miss_titles = df[df[\"label\"] == 0][\"title\"].str.lower()\n",
    "        \n",
    "        hit_words = Counter()\n",
    "        miss_words = Counter()\n",
    "        for title in hit_titles:\n",
    "            hit_words.update(title.split())\n",
    "        for title in miss_titles:\n",
    "            miss_words.update(title.split())\n",
    "        \n",
    "        for word in set(hit_words.keys()) | set(miss_words.keys()):\n",
    "            h = hit_words.get(word, 0)\n",
    "            m = miss_words.get(word, 0)\n",
    "            if h + m >= 50:  # Higher threshold to reduce overfitting\n",
    "                self.word_hit_rates[word] = h / (h + m)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Extract features for prediction.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Title length features\n",
    "        features[\"title_length\"] = df[\"title\"].str.len()\n",
    "        features[\"title_words\"] = df[\"title\"].str.split().str.len()\n",
    "        features[\"avg_word_length\"] = features[\"title_length\"] / (features[\"title_words\"] + 1)\n",
    "        \n",
    "        # Title patterns (content-based, not identity-based)\n",
    "        features[\"has_show_hn\"] = df[\"title\"].str.contains(\"Show HN\", case=False).astype(int)\n",
    "        features[\"has_ask_hn\"] = df[\"title\"].str.contains(\"Ask HN\", case=False).astype(int)\n",
    "        features[\"has_question\"] = df[\"title\"].str.contains(r\"\\?\", regex=True).astype(int)\n",
    "        features[\"has_numbers\"] = df[\"title\"].str.contains(r\"\\d+\", regex=True).astype(int)\n",
    "        features[\"has_year\"] = df[\"title\"].str.contains(r\"20[12]\\d\", regex=True).astype(int)\n",
    "        features[\"starts_capital\"] = df[\"title\"].str[0].str.isupper().astype(int)\n",
    "        features[\"all_caps_ratio\"] = df[\"title\"].apply(\n",
    "            lambda t: sum(1 for c in t if c.isupper()) / (len(t) + 1)\n",
    "        )\n",
    "        \n",
    "        # Punctuation features\n",
    "        features[\"has_colon\"] = df[\"title\"].str.contains(\":\").astype(int)\n",
    "        features[\"has_dash\"] = df[\"title\"].str.contains(\" - |â€“|â€”\").astype(int)\n",
    "        features[\"has_quotes\"] = df[\"title\"].str.contains(r'[\"\\']').astype(int)\n",
    "        features[\"has_parens\"] = df[\"title\"].str.contains(r\"\\(.*\\)\").astype(int)\n",
    "        \n",
    "        # Word-level features (aggregated, less leaky)\n",
    "        def calc_word_score(title):\n",
    "            words = title.lower().split()\n",
    "            scores = [self.word_hit_rates.get(w, 0.5) for w in words]\n",
    "            if not scores:\n",
    "                return 0.5, 0.5, 0.5\n",
    "            return np.mean(scores), np.max(scores), np.min(scores)\n",
    "        \n",
    "        word_scores = df[\"title\"].apply(calc_word_score)\n",
    "        features[\"word_hit_mean\"] = word_scores.apply(lambda x: x[0])\n",
    "        features[\"word_hit_max\"] = word_scores.apply(lambda x: x[1])\n",
    "        features[\"word_hit_min\"] = word_scores.apply(lambda x: x[2])\n",
    "        \n",
    "        # Time features (cyclical encoding) - these are OK, not identity-based\n",
    "        if \"timestamp\" in df.columns:\n",
    "            ts = pd.to_datetime(df[\"timestamp\"])\n",
    "            features[\"hour_sin\"] = np.sin(2 * np.pi * ts.dt.hour / 24)\n",
    "            features[\"hour_cos\"] = np.cos(2 * np.pi * ts.dt.hour / 24)\n",
    "            features[\"dow_sin\"] = np.sin(2 * np.pi * ts.dt.dayofweek / 7)\n",
    "            features[\"dow_cos\"] = np.cos(2 * np.pi * ts.dt.dayofweek / 7)\n",
    "            features[\"is_weekend\"] = ts.dt.dayofweek.isin([5, 6]).astype(int)\n",
    "        else:\n",
    "            features[\"hour_sin\"] = 0\n",
    "            features[\"hour_cos\"] = 1\n",
    "            features[\"dow_sin\"] = 0\n",
    "            features[\"dow_cos\"] = 1\n",
    "            features[\"is_weekend\"] = 0\n",
    "        \n",
    "        # URL type features (category, not identity)\n",
    "        features[\"has_url\"] = (df[\"url\"].str.len() > 0).astype(int)\n",
    "        features[\"is_github\"] = df[\"url\"].str.contains(\"github\", case=False).fillna(False).astype(int)\n",
    "        features[\"is_medium\"] = df[\"url\"].str.contains(r\"medium\\.com\", case=False, regex=True).fillna(False).astype(int)\n",
    "        features[\"is_twitter\"] = df[\"url\"].str.contains(r\"twitter|x\\.com\", case=False, regex=True).fillna(False).astype(int)\n",
    "        features[\"is_youtube\"] = df[\"url\"].str.contains(r\"youtube|youtu\\.be\", case=False, regex=True).fillna(False).astype(int)\n",
    "        features[\"is_arxiv\"] = df[\"url\"].str.contains(r\"arxiv\\.org\", case=False, regex=True).fillna(False).astype(int)\n",
    "        \n",
    "        return features.fillna(0)\n",
    "\n",
    "print(\"âœ“ FeatureEngineer class defined (v3.1 - no domain/author leakage)\")\n",
    "print(\"  Removed: domain_hit_rate, author_hit_rate, domain_count, author_count\")\n",
    "print(\"  Kept: title patterns, word scores, time features, URL categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation/test split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=SEED, stratify=df[\"label\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"label\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")\n",
    "\n",
    "# Fit feature engineer on training data\n",
    "feature_engineer = FeatureEngineer()\n",
    "feature_engineer.fit(train_df)\n",
    "\n",
    "# Transform all sets\n",
    "train_features = feature_engineer.transform(train_df)\n",
    "val_features = feature_engineer.transform(val_df)\n",
    "test_features = feature_engineer.transform(test_df)\n",
    "\n",
    "print(f\"\\nâœ“ Engineered {train_features.shape[1]} features\")\n",
    "print(f\"Features: {list(train_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Focal Loss & RoBERTa Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss: FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Key insight: gamma > 0 down-weights easy examples, focusing on hard ones.\n",
    "    alpha < 0.5 penalizes false positives more (we want fewer FP).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=2).float()\n",
    "        \n",
    "        # Get probability for correct class\n",
    "        pt = (probs * targets_one_hot).sum(dim=-1)\n",
    "        \n",
    "        # Alpha weighting (alpha for positive class, 1-alpha for negative)\n",
    "        alpha_t = self.alpha * targets.float() + (1 - self.alpha) * (1 - targets.float())\n",
    "        \n",
    "        # Focal term: (1 - pt)^gamma\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Cross entropy\n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        \n",
    "        # Combined focal loss\n",
    "        loss = alpha_t * focal_weight * ce_loss\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "print(f\"âœ“ FocalLoss defined (alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossTrainer(Trainer):\n",
    "    \"\"\"Custom trainer using Focal Loss instead of CrossEntropy.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, focal_alpha=0.25, focal_gamma=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.focal_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"âœ“ FocalLossTrainer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data for RoBERTa\n",
    "print(\"Tokenizing data...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_data(texts, labels):\n",
    "    tokens = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokens[\"labels\"] = labels.tolist()\n",
    "    return Dataset.from_dict(tokens)\n",
    "\n",
    "train_dataset = tokenize_data(train_df[\"title\"], train_df[\"label\"])\n",
    "val_dataset = tokenize_data(val_df[\"title\"], val_df[\"label\"])\n",
    "test_dataset = tokenize_data(test_df[\"title\"], test_df[\"label\"])\n",
    "\n",
    "train_labels = np.array(train_df[\"label\"])\n",
    "val_labels = np.array(val_df[\"label\"])\n",
    "test_labels = np.array(test_df[\"label\"])\n",
    "\n",
    "print(f\"âœ“ Tokenized: train={len(train_dataset)}, val={len(val_dataset)}, test={len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RoBERTa with Focal Loss\n",
    "print(\"Training RoBERTa with Focal Loss...\")\n",
    "print(f\"Focal Loss: alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_focal\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1)[:, 1].numpy()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"roc_auc\": roc_auc_score(labels, probs),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds)\n",
    "    }\n",
    "\n",
    "trainer = FocalLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    focal_alpha=FOCAL_ALPHA,\n",
    "    focal_gamma=FOCAL_GAMMA\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"\\nâœ“ RoBERTa training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RoBERTa predictions for stacking\n",
    "def get_roberta_predictions(dataset, batch_size=64):\n",
    "    \"\"\"Get probability predictions from RoBERTa.\"\"\"\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    \n",
    "    # Use HuggingFace's built-in batching\n",
    "    dataset_no_labels = dataset.remove_columns([\"labels\"])\n",
    "    dataset_no_labels.set_format(\"torch\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(dataset_no_labels), batch_size), desc=\"RoBERTa predictions\"):\n",
    "            batch = dataset_no_labels[i:i+batch_size]\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            batch_probs = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
    "            probs.extend(batch_probs)\n",
    "    \n",
    "    return np.array(probs)\n",
    "\n",
    "train_roberta_probs = get_roberta_predictions(train_dataset)\n",
    "val_roberta_probs = get_roberta_predictions(val_dataset)\n",
    "test_roberta_probs = get_roberta_predictions(test_dataset)\n",
    "\n",
    "print(f\"\\nâœ“ RoBERTa predictions extracted\")\n",
    "print(f\"  Train ROC AUC: {roc_auc_score(train_labels, train_roberta_probs):.4f}\")\n",
    "print(f\"  Val ROC AUC:   {roc_auc_score(val_labels, val_roberta_probs):.4f}\")\n",
    "print(f\"  Test ROC AUC:  {roc_auc_score(test_labels, test_roberta_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TF-IDF + Logistic Regression\n",
    "print(\"Training TF-IDF model...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=5)\n",
    "train_tfidf = tfidf.fit_transform(train_df[\"title\"])\n",
    "val_tfidf = tfidf.transform(val_df[\"title\"])\n",
    "test_tfidf = tfidf.transform(test_df[\"title\"])\n",
    "\n",
    "tfidf_model = LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=0.5, random_state=SEED)\n",
    "tfidf_model.fit(train_tfidf, train_labels)\n",
    "\n",
    "train_tfidf_probs = tfidf_model.predict_proba(train_tfidf)[:, 1]\n",
    "val_tfidf_probs = tfidf_model.predict_proba(val_tfidf)[:, 1]\n",
    "test_tfidf_probs = tfidf_model.predict_proba(test_tfidf)[:, 1]\n",
    "\n",
    "print(f\"\\nâœ“ TF-IDF model trained\")\n",
    "print(f\"  Train ROC AUC: {roc_auc_score(train_labels, train_tfidf_probs):.4f}\")\n",
    "print(f\"  Val ROC AUC:   {roc_auc_score(val_labels, val_tfidf_probs):.4f}\")\n",
    "print(f\"  Test ROC AUC:  {roc_auc_score(test_labels, test_tfidf_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LightGBM Stacking Meta-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features for stacking\n",
    "def build_stacking_features(roberta_probs, tfidf_probs, engineered_features):\n",
    "    \"\"\"Combine all feature sources for the meta-learner.\"\"\"\n",
    "    return np.column_stack([\n",
    "        roberta_probs,\n",
    "        tfidf_probs,\n",
    "        engineered_features.values\n",
    "    ])\n",
    "\n",
    "X_train_stack = build_stacking_features(train_roberta_probs, train_tfidf_probs, train_features)\n",
    "X_val_stack = build_stacking_features(val_roberta_probs, val_tfidf_probs, val_features)\n",
    "X_test_stack = build_stacking_features(test_roberta_probs, test_tfidf_probs, test_features)\n",
    "\n",
    "feature_names = [\"roberta_prob\", \"tfidf_prob\"] + list(train_features.columns)\n",
    "\n",
    "print(f\"âœ“ Stacking features: {X_train_stack.shape[1]} dimensions\")\n",
    "print(f\"  - RoBERTa prob: 1\")\n",
    "print(f\"  - TF-IDF prob: 1\")\n",
    "print(f\"  - Engineered: {train_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM stacking meta-learner\n",
    "print(\"Training LightGBM stacking meta-learner...\")\n",
    "print(f\"Parameters: {LGBM_PARAMS}\")\n",
    "\n",
    "lgbm_train = lgb.Dataset(X_train_stack, label=train_labels, feature_name=feature_names)\n",
    "lgbm_val = lgb.Dataset(X_val_stack, label=val_labels, feature_name=feature_names, reference=lgbm_train)\n",
    "\n",
    "stacker = lgb.train(\n",
    "    LGBM_PARAMS,\n",
    "    lgbm_train,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[lgbm_train, lgbm_val],\n",
    "    valid_names=[\"train\", \"val\"],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# Get raw predictions (before calibration)\n",
    "train_stacker_raw = stacker.predict(X_train_stack)\n",
    "val_stacker_raw = stacker.predict(X_val_stack)\n",
    "test_stacker_raw = stacker.predict(X_test_stack)\n",
    "\n",
    "print(f\"\\nâœ“ LightGBM Stacker trained\")\n",
    "print(f\"  Best iteration: {stacker.best_iteration}\")\n",
    "print(f\"  Val ROC AUC (raw): {roc_auc_score(val_labels, val_stacker_raw):.4f}\")\n",
    "print(f\"  Test ROC AUC (raw): {roc_auc_score(test_labels, test_stacker_raw):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Isotonic Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Isotonic Regression calibration\n",
    "print(\"Applying Isotonic Regression calibration...\")\n",
    "\n",
    "calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "calibrator.fit(val_stacker_raw, val_labels)\n",
    "\n",
    "# Calibrated predictions\n",
    "train_probs = calibrator.predict(train_stacker_raw)\n",
    "val_probs = calibrator.predict(val_stacker_raw)\n",
    "test_probs = calibrator.predict(test_stacker_raw)\n",
    "\n",
    "# Calculate calibration improvement\n",
    "def calculate_ece(probs, labels, n_bins=10):\n",
    "    \"\"\"Calculate Expected Calibration Error.\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (probs > bin_boundaries[i]) & (probs <= bin_boundaries[i+1])\n",
    "        if in_bin.sum() > 0:\n",
    "            bin_accuracy = labels[in_bin].mean()\n",
    "            bin_confidence = probs[in_bin].mean()\n",
    "            ece += (in_bin.sum() / len(probs)) * abs(bin_accuracy - bin_confidence)\n",
    "    return ece\n",
    "\n",
    "ece_before = calculate_ece(test_stacker_raw, test_labels)\n",
    "ece_after = calculate_ece(test_probs, test_labels)\n",
    "\n",
    "print(f\"\\nâœ“ Isotonic calibration applied\")\n",
    "print(f\"  ECE before calibration: {ece_before:.4f}\")\n",
    "print(f\"  ECE after calibration: {ece_after:.4f}\")\n",
    "print(f\"  Improvement: {((ece_before - ece_after) / ece_before * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Threshold Optimization & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize classification threshold\n",
    "print(\"Optimizing classification threshold...\")\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(test_labels, test_probs)\n",
    "\n",
    "# Find threshold that maximizes F1\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 0.5\n",
    "\n",
    "# Find threshold for balanced precision/recall\n",
    "balanced_diff = np.abs(precisions[:-1] - recalls[:-1])\n",
    "balanced_idx = np.argmin(balanced_diff)\n",
    "balanced_threshold = thresholds[balanced_idx]\n",
    "\n",
    "# Find threshold for target precision (0.6)\n",
    "target_precision = 0.6\n",
    "valid_mask = recalls[:-1] > 0.3\n",
    "precision_diff = np.abs(precisions[:-1] - target_precision)\n",
    "precision_diff[~valid_mask] = np.inf\n",
    "target_idx = np.argmin(precision_diff)\n",
    "precision_threshold = thresholds[target_idx]\n",
    "\n",
    "print(f\"\\nThreshold Analysis:\")\n",
    "print(f\"  Best F1 threshold: {best_f1_threshold:.3f}\")\n",
    "print(f\"    -> F1={f1_scores[best_f1_idx]:.3f}, P={precisions[best_f1_idx]:.3f}, R={recalls[best_f1_idx]:.3f}\")\n",
    "print(f\"  Balanced P/R threshold: {balanced_threshold:.3f}\")\n",
    "print(f\"    -> P={precisions[balanced_idx]:.3f}, R={recalls[balanced_idx]:.3f}\")\n",
    "print(f\"  Precision {target_precision:.0%} threshold: {precision_threshold:.3f}\")\n",
    "print(f\"    -> P={precisions[target_idx]:.3f}, R={recalls[target_idx]:.3f}\")\n",
    "\n",
    "# Use balanced threshold as default\n",
    "OPTIMAL_THRESHOLD = balanced_threshold\n",
    "print(f\"\\nâœ“ Selected threshold: {OPTIMAL_THRESHOLD:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation report\n",
    "print(\"=\" * 60)\n",
    "print(\"V3 STACKING META-LEARNER - FULL EVALUATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_preds = (test_probs >= OPTIMAL_THRESHOLD).astype(int)\n",
    "\n",
    "# Core metrics\n",
    "roc_auc = roc_auc_score(test_labels, test_probs)\n",
    "avg_precision = average_precision_score(test_labels, test_probs)\n",
    "precision = precision_score(test_labels, test_preds)\n",
    "recall = recall_score(test_labels, test_preds)\n",
    "f1 = f1_score(test_labels, test_preds)\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"\\nðŸ“Š CORE METRICS (threshold={OPTIMAL_THRESHOLD:.3f}):\")\n",
    "print(f\"  ROC AUC:           {roc_auc:.4f}\")\n",
    "print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"  Precision:         {precision:.4f}\")\n",
    "print(f\"  Recall:            {recall:.4f}\")\n",
    "print(f\"  F1 Score:          {f1:.4f}\")\n",
    "print(f\"  Accuracy:          {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(test_labels, test_preds).ravel()\n",
    "print(f\"\\nðŸ“‹ CONFUSION MATRIX:\")\n",
    "print(f\"  True Negatives:  {tn:,}\")\n",
    "print(f\"  False Positives: {fp:,}\")\n",
    "print(f\"  False Negatives: {fn:,}\")\n",
    "print(f\"  True Positives:  {tp:,}\")\n",
    "print(f\"  FP Rate: {fp/(fp+tn):.2%}\")\n",
    "print(f\"  FN Rate: {fn/(fn+tp):.2%}\")\n",
    "\n",
    "# Calibration\n",
    "print(f\"\\nðŸŽ¯ CALIBRATION:\")\n",
    "print(f\"  ECE (Expected Calibration Error): {ece_after:.4f}\")\n",
    "\n",
    "# Comparison with v2\n",
    "print(f\"\\nðŸ“ˆ COMPARISON WITH V2:\")\n",
    "v2_roc = 0.6984\n",
    "v2_precision = 0.4173\n",
    "v2_fp = 2847\n",
    "v2_ece = 0.1135\n",
    "print(f\"  ROC AUC:   {v2_roc:.4f} -> {roc_auc:.4f} ({'â†‘' if roc_auc > v2_roc else 'â†“'} {abs(roc_auc - v2_roc):.4f})\")\n",
    "print(f\"  Precision: {v2_precision:.4f} -> {precision:.4f} ({'â†‘' if precision > v2_precision else 'â†“'} {abs(precision - v2_precision):.4f})\")\n",
    "print(f\"  FP Count:  {v2_fp:,} -> {fp:,} ({'â†“' if fp < v2_fp else 'â†‘'} {abs(fp - v2_fp):,})\")\n",
    "print(f\"  ECE:       {v2_ece:.4f} -> {ece_after:.4f} ({'â†“' if ece_after < v2_ece else 'â†‘'} {abs(ece_after - v2_ece):.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Feature Importance Analysis (LightGBM Stacker):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "importance = stacker.feature_importance(importance_type=\"gain\")\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importance\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "for _, row in importance_df.head(15).iterrows():\n",
    "    bar = \"â–ˆ\" * int(row[\"importance\"] / importance_df[\"importance\"].max() * 20)\n",
    "    print(f\"  {row['feature'][:25]:<25} {bar} {row['importance']:.1f}\")\n",
    "\n",
    "# Model contribution analysis\n",
    "print(\"\\n\\nModel Contribution Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "roberta_imp = importance_df[importance_df[\"feature\"] == \"roberta_prob\"][\"importance\"].values[0]\n",
    "tfidf_imp = importance_df[importance_df[\"feature\"] == \"tfidf_prob\"][\"importance\"].values[0]\n",
    "eng_imp = importance_df[~importance_df[\"feature\"].isin([\"roberta_prob\", \"tfidf_prob\"])][\"importance\"].sum()\n",
    "total_imp = roberta_imp + tfidf_imp + eng_imp\n",
    "\n",
    "print(f\"  RoBERTa probability:    {roberta_imp/total_imp*100:5.1f}%\")\n",
    "print(f\"  TF-IDF probability:     {tfidf_imp/total_imp*100:5.1f}%\")\n",
    "print(f\"  Engineered features:    {eng_imp/total_imp*100:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. ROC Curve\n",
    "ax1 = axes[0, 0]\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_probs)\n",
    "ax1.plot(fpr, tpr, \"b-\", linewidth=2, label=f\"V3 Stacker (AUC={roc_auc:.3f})\")\n",
    "ax1.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
    "ax1.fill_between(fpr, tpr, alpha=0.2)\n",
    "ax1.set_xlabel(\"False Positive Rate\")\n",
    "ax1.set_ylabel(\"True Positive Rate\")\n",
    "ax1.set_title(\"ROC Curve - V3 Stacking Meta-Learner\")\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(recalls, precisions[:-1], \"g-\", linewidth=2, label=f\"V3 (AP={avg_precision:.3f})\")\n",
    "ax2.axhline(y=0.5, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Random (AP=0.5)\")\n",
    "ax2.scatter([recall], [precision], color=\"red\", s=100, zorder=5, label=f\"Operating point\")\n",
    "ax2.set_xlabel(\"Recall\")\n",
    "ax2.set_ylabel(\"Precision\")\n",
    "ax2.set_title(\"Precision-Recall Curve\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Calibration Plot\n",
    "ax3 = axes[1, 0]\n",
    "n_bins = 10\n",
    "bin_means, bin_true = [], []\n",
    "for i in range(n_bins):\n",
    "    low, high = i / n_bins, (i + 1) / n_bins\n",
    "    mask = (test_probs >= low) & (test_probs < high)\n",
    "    if mask.sum() > 0:\n",
    "        bin_means.append(test_probs[mask].mean())\n",
    "        bin_true.append(test_labels[mask].mean())\n",
    "\n",
    "ax3.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "ax3.scatter(bin_means, bin_true, s=100, alpha=0.7, label=\"V3 Calibrated\")\n",
    "ax3.plot(bin_means, bin_true, \"b-\", alpha=0.5)\n",
    "ax3.set_xlabel(\"Mean Predicted Probability\")\n",
    "ax3.set_ylabel(\"Fraction of Positives\")\n",
    "ax3.set_title(f\"Calibration Plot (ECE={ece_after:.4f})\")\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "ax4 = axes[1, 1]\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\",\", cmap=\"Blues\", ax=ax4,\n",
    "            xticklabels=[\"Predicted 0\", \"Predicted 1\"],\n",
    "            yticklabels=[\"Actual 0\", \"Actual 1\"])\n",
    "ax4.set_title(f\"Confusion Matrix (threshold={OPTIMAL_THRESHOLD:.2f})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"v3_evaluation_plots.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"\\nâœ“ Plots saved to v3_evaluation_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Demo & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo predictions on sample titles\n",
    "print(\"Demo Predictions on Sample Titles:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sample_titles = [\n",
    "    \"Show HN: I built a real-time collaborative code editor\",\n",
    "    \"Why I'm mass-deleting my old tweets\",\n",
    "    \"The hidden costs of serverless architectures\",\n",
    "    \"Ask HN: What are you working on this weekend?\",\n",
    "    \"GPT-5 achieves human-level performance on coding benchmarks\",\n",
    "    \"How we reduced our AWS bill by 80%\",\n",
    "    \"A random blog post about my cat\",\n",
    "    \"The Rust programming language is overrated\",\n",
    "    \"New research shows coffee may extend lifespan\",\n",
    "    \"I accidentally deleted production database, here's what happened\"\n",
    "]\n",
    "\n",
    "# Create minimal dataframe for prediction\n",
    "sample_df = pd.DataFrame({\"title\": sample_titles})\n",
    "sample_df[\"url\"] = \"\"\n",
    "sample_df[\"submitter\"] = \"unknown\"\n",
    "sample_df[\"timestamp\"] = pd.Timestamp.now()\n",
    "\n",
    "# Get features for samples\n",
    "sample_tokens = tokenizer(sample_titles, truncation=True, padding=\"max_length\", \n",
    "                          max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "# RoBERTa predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in sample_tokens.items()}\n",
    "    outputs = model(**inputs)\n",
    "    sample_roberta_probs = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()\n",
    "\n",
    "# TF-IDF predictions\n",
    "sample_tfidf_probs = tfidf_model.predict_proba(tfidf.transform(sample_titles))[:, 1]\n",
    "\n",
    "# Engineered features\n",
    "sample_eng = feature_engineer.transform(sample_df)\n",
    "\n",
    "# Stack and predict\n",
    "sample_stack = build_stacking_features(sample_roberta_probs, sample_tfidf_probs, sample_eng)\n",
    "sample_raw = stacker.predict(sample_stack)\n",
    "sample_probs = calibrator.predict(sample_raw)\n",
    "\n",
    "print(f\"{'Title':<55} {'Prob':>6} {'Pred':>6}\")\n",
    "print(\"-\" * 70)\n",
    "for title, prob in zip(sample_titles, sample_probs):\n",
    "    pred = \"âœ… HIT\" if prob >= OPTIMAL_THRESHOLD else \"âŒ MISS\"\n",
    "    title_short = title[:52] + \"...\" if len(title) > 55 else title\n",
    "    print(f\"{title_short:<55} {prob:>5.1%} {pred:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all model components\n",
    "print(\"Saving model components...\")\n",
    "os.makedirs(\"./hn_model_v3\", exist_ok=True)\n",
    "\n",
    "# Save RoBERTa model\n",
    "model.save_pretrained(\"./hn_model_v3/roberta\")\n",
    "tokenizer.save_pretrained(\"./hn_model_v3/roberta\")\n",
    "print(\"âœ“ RoBERTa model saved\")\n",
    "\n",
    "# Save TF-IDF + LogReg\n",
    "joblib.dump(tfidf, \"./hn_model_v3/tfidf_vectorizer.joblib\")\n",
    "joblib.dump(tfidf_model, \"./hn_model_v3/tfidf_model.joblib\")\n",
    "print(\"âœ“ TF-IDF model saved\")\n",
    "\n",
    "# Save LightGBM stacker\n",
    "stacker.save_model(\"./hn_model_v3/lgbm_stacker.txt\")\n",
    "print(\"âœ“ LightGBM stacker saved\")\n",
    "\n",
    "# Save calibrator\n",
    "joblib.dump(calibrator, \"./hn_model_v3/isotonic_calibrator.joblib\")\n",
    "print(\"âœ“ Isotonic calibrator saved\")\n",
    "\n",
    "# Save feature engineer\n",
    "joblib.dump(feature_engineer, \"./hn_model_v3/feature_engineer.joblib\")\n",
    "print(\"âœ“ Feature engineer saved\")\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    \"version\": \"v3\",\n",
    "    \"architecture\": \"RoBERTa + TF-IDF + Engineered Features -> LightGBM Stacker -> Isotonic Calibration\",\n",
    "    \"optimal_threshold\": float(OPTIMAL_THRESHOLD),\n",
    "    \"focal_loss\": {\"alpha\": FOCAL_ALPHA, \"gamma\": FOCAL_GAMMA},\n",
    "    \"lgbm_params\": LGBM_PARAMS,\n",
    "    \"metrics\": {\n",
    "        \"roc_auc\": float(roc_auc),\n",
    "        \"average_precision\": float(avg_precision),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"ece\": float(ece_after)\n",
    "    },\n",
    "    \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
    "    \"feature_names\": feature_names\n",
    "}\n",
    "\n",
    "with open(\"./hn_model_v3/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"âœ“ Configuration saved\")\n",
    "\n",
    "# Create zip for download\n",
    "!cd hn_model_v3 && zip -r ../hn_model_v3.zip .\n",
    "print(\"\\nâœ“ All components saved to hn_model_v3.zip\")\n",
    "\n",
    "# List files\n",
    "!ls -la hn_model_v3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model zip\n",
    "from google.colab import files\n",
    "files.download(\"hn_model_v3.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
